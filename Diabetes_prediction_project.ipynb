{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Diabetes Prediction\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "Made By- Vaibhav Verma"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diabetes Prediction using Machine Learning\n",
        "\n",
        "Diabetes, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period. Symptoms of high blood sugar include frequent urination, increased thirst, and increased hunger. If left untreated, diabetes can cause many complications. Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death. Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, and damage to the eyes.\n",
        "\n",
        "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "import statsmodels.api as sm\n",
        "from collections import Counter\n",
        "import os\n",
        "from sklearn.preprocessing import scale, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import warnings\n",
        "warnings.simplefilter(action = \"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import files\n",
        "uploaded =files.upload()"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df = pd.read_csv('/content/diabetes.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df= pd.read_csv('/content/diabetes.csv')\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Ly9tz2yr3orT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "num_rows, num_cols = df.shape\n",
        "print(\"Number of rows:\", num_rows)\n",
        "print(\"Number of columns:\", num_cols)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_rows = df[df.duplicated()]\n",
        "print(duplicate_rows)\n",
        "num_duplicate_rows = len(duplicate_rows)\n",
        "print(num_duplicate_rows)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values_count = df.isnull().sum()\n",
        "print(missing_values_count)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "df[\"Age\"].hist(edgecolor = \"black\");"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms provide a clear visual representation of the distribution of a continuous variable. They show the frequency or density of data within predefined intervals (bins), making it easy to identify patterns, trends, and outliers in the data."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "counts = df['Outcome'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(9, 9))  # Set the size of the pie chart\n",
        "plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('target')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "import seaborn as sns\n",
        "sns.boxplot(x = df[\"Insulin\"]);"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Exploring Pregnancy and target variables together\n",
        "\n",
        "plt.figure(figsize = (10, 8))\n",
        "\n",
        "# Plotting density function graph of the pregnancies and the target variable\n",
        "kde = sns.kdeplot(df[\"Pregnancies\"][df[\"Outcome\"] == 1], color = \"Red\", shade = True)\n",
        "kde = sns.kdeplot(df[\"Pregnancies\"][df[\"Outcome\"] == 0], ax = kde, color = \"Blue\", shade= True)\n",
        "kde.set_xlabel(\"Pregnancies\")\n",
        "kde.set_ylabel(\"Density\")\n",
        "kde.legend([\"Positive Result\", \"Negative Result\"])"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Exploring the Glucose and the Target variables together\n",
        "plt.figure(figsize = (10, 8))\n",
        "sns.violinplot(data = df, x = \"Outcome\", y = \"Glucose\",\n",
        "               split = True, inner = \"quart\", linewidth = 2)"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x='Pregnancies', hue='Age')\n",
        "plt.title('Distribution of regnancy rate')\n",
        "plt.xlabel('pregnancies')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "sns.kdeplot(data=df, x=\"SkinThickness\",hue=\"Age\",fill=True,alpha=0.8, linewidth=1.5)"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "f, ax = plt.subplots(figsize= [15,10])\n",
        "sns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"magma\" )\n",
        "ax.set_title(\"Correlation Matrix\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10\n"
      ],
      "metadata": {
        "id": "RIeKLdWf_dJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x= \"Insulin\", y =\"Age\", data = df)"
      ],
      "metadata": {
        "id": "e1Dxs4SZ_sZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we can look at where are missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "nflheJ1fAh6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Have been visualized using the missingno library for the visualization of missing observations.\n",
        "# Plotting\n",
        "import missingno as msno\n",
        "msno.bar(df);"
      ],
      "metadata": {
        "id": "oMJUN6wGA58q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The missing values ​​will be filled with the median values ​​of each variable.\n",
        "def median_target(var):\n",
        "    temp = df[df[var].notnull()]\n",
        "    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n",
        "    return temp\n",
        "    # The values to be given for incomplete observations are given the median value of people who are not sick and the median values of people who are sick.\n",
        "columns = df.columns\n",
        "columns = columns.drop(\"Outcome\")\n",
        "for i in columns:\n",
        "    median_target(i)\n",
        "    df.loc[(df['Outcome'] == 0 ) & (df[i].isnull()), i] = median_target(i)[i][0]\n",
        "    df.loc[(df['Outcome'] == 1 ) & (df[i].isnull()), i] = median_target(i)[i][1]\n",
        "    df.head()"
      ],
      "metadata": {
        "id": "Qhd2D_jpA-Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values were filled.\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GDomD0WiBQS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# In the data set, there were asked whether there were any outlier observations compared to the 25% and 75% quarters.\n",
        "# It was found to be an outlier observation.\n",
        "for feature in df:\n",
        "\n",
        "    Q1 = df[feature].quantile(0.25)\n",
        "    Q3 = df[feature].quantile(0.75)\n",
        "    IQR = Q3-Q1\n",
        "    lower = Q1- 1.5*IQR\n",
        "    upper = Q3 + 1.5*IQR\n",
        "\n",
        "    if df[(df[feature] > upper)].any(axis=None):\n",
        "        print(feature,\"yes\")\n",
        "    else:\n",
        "        print(feature, \"no\")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The process of visualizing the Insulin variable with boxplot method was done. We find the outlier observations on the chart.\n",
        "import seaborn as sns\n",
        "sns.boxplot(x = df[\"Insulin\"]);"
      ],
      "metadata": {
        "id": "IYbuN1SYoJIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We determine outliers between all variables with the LOF method\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "lof =LocalOutlierFactor(n_neighbors= 10)\n",
        "lof.fit_predict(df)\n"
      ],
      "metadata": {
        "id": "tHRxb2NLC-Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores = lof.negative_outlier_factor_\n",
        "np.sort(df_scores)[0:30]"
      ],
      "metadata": {
        "id": "yaFMsBLtDGJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We choose the threshold value according to lof scores\n",
        "threshold = np.sort(df_scores)[7]\n",
        "threshold"
      ],
      "metadata": {
        "id": "kExlCe8GDJYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We delete those that are higher than the threshold\n",
        "outlier = df_scores > threshold\n",
        "df = df[outlier]"
      ],
      "metadata": {
        "id": "Yct0q6I6DM2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "QyB6ecZHDPMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# According to BMI, some ranges were determined and categorical variables were assigned.\n",
        "NewBMI = pd.Series([\"Underweight\", \"Normal\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"], dtype = \"category\")\n",
        "df[\"NewBMI\"] = NewBMI\n",
        "df.loc[df[\"BMI\"] < 18.5, \"NewBMI\"] = NewBMI[0]\n",
        "df.loc[(df[\"BMI\"] > 18.5) & (df[\"BMI\"] <= 24.9), \"NewBMI\"] = NewBMI[1]\n",
        "df.loc[(df[\"BMI\"] > 24.9) & (df[\"BMI\"] <= 29.9), \"NewBMI\"] = NewBMI[2]\n",
        "df.loc[(df[\"BMI\"] > 29.9) & (df[\"BMI\"] <= 34.9), \"NewBMI\"] = NewBMI[3]\n",
        "df.loc[(df[\"BMI\"] > 34.9) & (df[\"BMI\"] <= 39.9), \"NewBMI\"] = NewBMI[4]\n",
        "df.loc[df[\"BMI\"] > 39.9 ,\"NewBMI\"] = NewBMI[5]"
      ],
      "metadata": {
        "id": "EvfI3fkdoyLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A categorical variable creation process is performed according to the insulin value.\n",
        "def set_insulin(row):\n",
        "    if row[\"Insulin\"] >= 16 and row[\"Insulin\"] <= 166:\n",
        "        return \"Normal\"\n",
        "    else:\n",
        "        return \"Abnormal\""
      ],
      "metadata": {
        "id": "iJq4fuYyo0vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The operation performed was added to the dataframe.\n",
        "df = df.assign(NewInsulinScore=df.apply(set_insulin, axis=1))\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KJaerdXgo598"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Some intervals were determined according to the glucose variable and these were assigned categorical variables.\n",
        "NewGlucose = pd.Series([\"Low\", \"Normal\", \"Overweight\", \"Secret\", \"High\"], dtype = \"category\")\n",
        "df[\"NewGlucose\"] = NewGlucose\n",
        "df.loc[df[\"Glucose\"] <= 70, \"NewGlucose\"] = NewGlucose[0]\n",
        "df.loc[(df[\"Glucose\"] > 70) & (df[\"Glucose\"] <= 99), \"NewGlucose\"] = NewGlucose[1]\n",
        "df.loc[(df[\"Glucose\"] > 99) & (df[\"Glucose\"] <= 126), \"NewGlucose\"] = NewGlucose[2]\n",
        "df.loc[df[\"Glucose\"] > 126 ,\"NewGlucose\"] = NewGlucose[3]"
      ],
      "metadata": {
        "id": "d9V7odj8o9pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, by making One Hot Encoding transformation, categorical variables were converted into numerical values. It is also protected from the Dummy variable trap.\n",
        "df = pd.get_dummies(df, columns =[\"NewBMI\",\"NewInsulinScore\", \"NewGlucose\"], drop_first = True)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "c8WzPAYDWe16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_df = df[['NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n",
        "                     'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret']]"
      ],
      "metadata": {
        "id": "q5druv-MqDDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_df.head()"
      ],
      "metadata": {
        "id": "KdbwM9zoqHAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df[\"Outcome\"]\n",
        "X = df.drop([\"Outcome\",'NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n",
        "                     'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret'], axis = 1)\n",
        "cols = X.columns\n",
        "index = X.index"
      ],
      "metadata": {
        "id": "hJ5PnqMnpmS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "transformer = RobustScaler().fit(X)\n",
        "X = transformer.transform(X)\n",
        "X = pd.DataFrame(X, columns = cols, index = index)"
      ],
      "metadata": {
        "id": "Uy7Q07_8prGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "kKe4EuHlpu5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.concat([X,categorical_df], axis = 1)"
      ],
      "metadata": {
        "id": "5YIAWwqEpw86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "Qa2KA2OXpzkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.pd.get_dummies() is a function in the pandas library in Python that is used to convert categorical variables into dummy/indicator variables. This process is also known as one-hot encoding.\n",
        "\n",
        "2.In One-hot encoding, a categorical variable is transformed into a binary matrix where each column represents a category and each row represents an observation. Each cell in the matrix is filled with a binary value of 1 or 0 indicating the presence or absence of a particular category in that observation."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "oaastl1aVCD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming the data into quartiles\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "quartile  = QuantileTransformer()\n",
        "X = quartile.fit_transform(df)\n",
        "dataset = quartile.transform(X)\n",
        "dataset = pd.DataFrame(X)\n",
        "dataset.columns =['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome','NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n",
        "                     'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret']\n",
        "# Showing the top 5 rows of the transformed dataset\n",
        "dataset.head()\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dependent and independent features\n",
        "X = df.drop([\"Outcome\"], axis = 1)\n",
        "Y = df[\"Outcome\"]\n",
        "\n",
        "# Splitting the dataset into the training and testing dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.40, random_state = 10)\n",
        "\n",
        "# Printing the size of the training and testing dataset\n",
        "print(\"The size of the training dataset: \", X_train.size)\n",
        "print(\"The size of the testing dataset: \", X_test.size)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "Cygn1_qSXajs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "Xntf1lTSXdJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the 40 to 60 ratio\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "The machine learning model used above is a Decision Tree Classifier. Decision trees are a type of supervised learning algorithm used for classification and regression tasks. In a decision tree, the data is split into subsets based on the values of features, with the aim of creating a tree-like model of decisions. At each node of the tree, the algorithm selects the feature that best splits the data into classes (in the case of classification) or minimizes the variance (in the case of regression)."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "clf = DecisionTreeClassifier(random_state=100)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(classification_report(Y_test, y_pred))"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=100)\n",
        "\n",
        "# Perform Grid Search Cross-Validation\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n",
        "\n",
        "# Initialize Decision Tree Classifier with best parameters\n",
        "best_clf = DecisionTreeClassifier(**best_params)\n",
        "\n",
        "# Evaluate using Cross-Validation\n",
        "cv_scores = cross_val_score(best_clf, X_train, Y_train, cv=5)\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Mean CV Score:\", cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Grid Search Cross-Validation for hyperparameter optimization.\n",
        "\n",
        "Grid Search Cross-Validation is a technique used to tune the hyperparameters of a machine learning model by exhaustively searching through a specified grid of hyperparameter values. It works by evaluating the model performance for each combination of hyperparameters using cross-validation and selecting the combination that yields the best performance according to a specified evaluation metric."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, after hyperparameter tuning, the model's accuracy increased from 0.85 to a mean cross-validation score of 0.86, indicating an improvement in performance. The best parameters found through grid search suggest that certain hyperparameter settings were more effective in optimizing the model's performance for the given dataset."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "The machine learning model used in the provided example is a k-Nearest Neighbors (kNN) classifier for predicting diabetes."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Initialize the kNN classifier\n",
        "k = 5  # Choose the value of k\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "# Train the classifier\n",
        "knn_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = knn_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(Y_test, y_pred))"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a range of k values to search\n",
        "param_grid = {'n_neighbors': range(1, 21)}  # Search for k values from 1 to 20\n",
        "\n",
        "# Initialize the kNN classifier\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(knn_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Perform grid search to find the best k value\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Print the best k value and corresponding accuracy\n",
        "print(\"Best k value:\", grid_search.best_params_['n_neighbors'])\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Re-train the classifier with the best k value\n",
        "best_k = grid_search.best_params_['n_neighbors']\n",
        "best_knn_classifier = KNeighborsClassifier(n_neighbors=best_k)\n",
        "best_knn_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the classifier using cross-validation\n",
        "cv_scores = cross_val_score(best_knn_classifier, X_train, Y_train, cv=5)\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_knn_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "print(\"Test Set Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I used Grid Search for hyperparameter optimization.\n",
        " it is suitable for relatively small parameter grids and datasets, making it a good choice for this scenario. Additionally, for more efficient search in larger spaces, techniques like Random Search or Bayesian Optimization can be considered."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you run the provided code on your dataset and compare the performance of the model before and after hyperparameter tuning, you should be able to observe any improvements in terms of accuracy or other evaluation metrics.The accuracy was 0.71 before hyperparameter, after tuning the acc. went to 0.79."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each metric provides valuable insights into different aspects of the model's performance, guiding businesses in making informed decisions and optimizing outcomes."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_reg = LogisticRegression(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "X = df.drop('Outcome', axis=1)  # Features\n",
        "Y = df['Outcome']  # Target variable\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "logistic_reg.fit(X_train, Y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = logistic_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(Y_test, y_pred))"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "Logistic regression is a classification algorithm used to model the probability of a binary outcome (in this case, whether an individual has diabetes or not) based on one or more predictor variables (features such as glucose levels, BMI, age, etc.).\n",
        "Model Training: The logistic regression model is trained on a labeled dataset containing features and corresponding binary labels indicating the presence or absence of diabetes.\n",
        "Output Interpretation: Logistic regression outputs probabilities between 0 and 1, representing the likelihood of belonging to the positive class (diabetic). These probabilities can be thresholded to make binary predictions."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Plotting the actual vs predicted values\n",
        "# Generate predictions\n",
        "y_pred = logistic_reg.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(Y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(Y_test, y_pred))\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Define hyperparameter grid\n",
        "from scipy.stats import uniform\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'C': uniform(loc=0, scale=4),  # Regularization parameter\n",
        "    'penalty': ['l1', 'l2']        # Penalty (L1 or L2 regularization)\n",
        "}\n",
        "\n",
        "# Initialize logistic regression classifier\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(log_reg, param_distributions=param_grid, n_iter=100, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "# Perform random search to find the best hyperparameters\n",
        "random_search.fit(X_train, Y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "# Re-train the model using the best hyperparameters\n",
        "best_log_reg = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the model using cross-validation\n",
        "cv_scores = cross_val_score(best_log_reg, X_train, Y_train, cv=5)\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "print(\"Test Set Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, Randomized Search Cross-Validation strikes a balance between exhaustively searching the entire hyperparameter space (as done in Grid Search) and sampling random points to efficiently find good hyperparameter configurations. It's particularly useful when there are many hyperparameters to tune or when computational resources are limited."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before Hyperparameter Tuning:\n",
        "\n",
        "Accuracy: 0.74\n",
        "Precision: 0.68\n",
        "Recall: 0.72\n",
        "F1-score: 0.70\n",
        "After Hyperparameter Tuning:\n",
        "\n",
        "Accuracy: 0.76\n",
        "Precision: 0.72\n",
        "Recall: 0.76\n",
        "F1-score: 0.74\n",
        "In this hypothetical scenario, we observe improvements in all evaluation metrics after hyperparameter tuning. The accuracy has increased from 0.74 to 0.75, indicating that a higher proportion of instances are correctly classified."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When considering evaluation metrics for a positive business impact, it's important to select metrics that directly align with the business objectives and goals. For a diabetes prediction task, where the goal might be to identify individuals at risk of diabetes, the following evaluation metrics could be considered:"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression model would be my preferred choice as the final prediction model for diabetes. It strikes a balance between performance, interpretability, and robustness, making it suitable for practical deployment in healthcare settings.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap\n"
      ],
      "metadata": {
        "id": "JHjarpOVDRCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Initialize SHAP explainer with the logistic regression model\n",
        "explainer = shap.Explainer(best_log_reg, X_train)\n",
        "\n",
        "# Calculate SHAP values\n",
        "shap_values = explainer.shap_values(X_train)\n",
        "\n",
        "# Plot feature importance\n",
        "shap.summary_plot(shap_values, X_train, plot_type=\"bar\", class_names=[\"Not Diabetic\", \"Diabetic\"])\n"
      ],
      "metadata": {
        "id": "m2DptG2-nulT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's delve into the logistic regression model used for diabetes prediction and explore feature importance using a popular model explainability tool called \"SHAP\" (SHapley Additive exPlanations).\n",
        "\n",
        "\n",
        "The resulting plot will show the feature importance values, indicating which features have the most significant impact on the model's predictions for both the \"Diabetic\" and \"Not Diabetic\" classes. Features with higher SHAP values are considered more important in influencing the model's predictions."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For diabetes prediction, where interpretability and simplicity are important considerations, logistic regression emerges as the preferred choice. Its ability to provide easily interpretable results, scalability, and well-established effectiveness in binary classification tasks make it a practical option for real-world deployment.\n",
        "\n",
        "While decision trees and kNN classifiers have their strengths, such as interpretability and ability to capture complex relationships, they may not offer the same level of interpretability and generalization performance as logistic regression for this specific task. Moreover, decision trees are prone to overfitting, and kNN can be computationally expensive.\n",
        "\n",
        "Therefore, based on the specific requirements and priorities of the diabetes prediction task, logistic regression stands out as the most suitable model choice."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}